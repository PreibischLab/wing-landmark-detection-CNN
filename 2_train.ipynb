{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tif\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "print(\"skimage\", skimage.__version__)\n",
    "## if skimage < 0.15 needs to manually import their resize (if using resize), for anti_aliasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"keras\", keras.__version__)\n",
    "#print(\"tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the backend the ordering of the channels\n",
    "print(keras.backend.backend())\n",
    "print(keras.backend.image_dim_ordering())\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('Greys');\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'YOURPATH' ## of augmented npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing different parameters:\n",
    "##  run the notebook with this cell.\n",
    "\n",
    "## Run the notebook as a parameterized script:\n",
    "\n",
    "## CUDA_VISIBLE_DEVICES=\"0\" KERAS_BACKEND=tensorflow WING_CNN_CSV=\"XXXX\" jupyter nbconvert --execute --ExecutePreprocessor.timeout=-1 1_CNN_wing_landmark_detection.ipynb\n",
    "\n",
    "\n",
    "# csv_folder = os.path.join(input_dir,'csv_dir')\n",
    "\n",
    "# with open(os.path.join(csv_folder,f'8.csv'), newline='') as csvfile:\n",
    "#     reader = csv.reader(csvfile, delimiter=',')\n",
    "#     params = {row[0]:row[1] for row in reader}\n",
    "    \n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also - only if running parallel - e.g to test different sigmas:\n",
    "\n",
    "# import tensorflow as tf\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_name = 'X.npy'\n",
    "Y_name = 'Y.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(os.path.join(input_dir, X_name))\n",
    "y = np.load(os.path.join(input_dir, Y_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Def NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conv = 64\n",
    "n_output_channels = y.shape[-1]\n",
    "dropout_rate = 0.25\n",
    "\n",
    "batch_size = 4\n",
    "epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(input_shape, n_output_channels):\n",
    "\n",
    "    x_in = Input(input_shape)\n",
    "\n",
    "    x1 = Conv2D(n_conv, kernel_size=3, padding=\"same\", activation=\"relu\")(x_in)\n",
    "    x1 = Conv2D(n_conv, kernel_size=3, padding=\"same\", activation=\"relu\")(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    \n",
    "    x1_pool = MaxPooling2D(pool_size=2, strides=2, padding=\"same\")(x1)\n",
    "    x1_pool = Dropout(0.25)(x1_pool)\n",
    "    \n",
    "    x2 = Conv2D(n_conv*2, kernel_size=3, padding=\"same\", activation=\"relu\")(x1_pool)\n",
    "    x2 = Conv2D(n_conv*2, kernel_size=3, padding=\"same\", activation=\"relu\")(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    \n",
    "    x2_pool = MaxPooling2D(pool_size=2, strides=2, padding=\"same\")(x2)\n",
    "    x2_pool = Dropout(0.25)(x2_pool)\n",
    "\n",
    "    x3 = Conv2D(n_conv*4, kernel_size=3, padding=\"same\", activation=\"relu\")(x2_pool)\n",
    "    x3 = Conv2D(n_conv*4, kernel_size=3, padding=\"same\", activation=\"relu\")(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "\n",
    "    x4 = Conv2DTranspose(n_conv*2, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(x3)\n",
    "    x4 = Conv2D(n_conv*2, kernel_size=3, padding=\"same\", activation=\"relu\")(x4)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "\n",
    "    x_out = Conv2DTranspose(n_output_channels, kernel_size=3, strides=2, padding=\"same\", activation=\"linear\")(x4)\n",
    "\n",
    "    # Compile\n",
    "    CNN = Model(inputs=x_in, outputs=x_out, name=\"CNN\")\n",
    "    CNN.compile(optimizer=Adam(), loss=\"mean_squared_error\")\n",
    "\n",
    "    return CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input, Shuffle, Split Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_set_shuffle_and_split(x, y, is_shuffle=True, shuffle_seed=0, n_train=20):\n",
    "    \n",
    "    if y.ndim == 3:\n",
    "        y = y.astype('float32')[:,:,:, None]\n",
    "        \n",
    "    x = x.astype('float32')[:,:,:, None]\n",
    "\n",
    "    print(f'x size: {x.shape}')\n",
    "    print(f'y size: {y.shape}')\n",
    "                             \n",
    "    input_shape = x.shape[1:] + (1,)\n",
    "    \n",
    "    # Check Y Range:\n",
    "    print(f'max Y: {np.max(y)}, min Y: {np.min(y)}')\n",
    "    \n",
    "    if is_shuffle:\n",
    "        x, y = shuffle(x, y, random_state=shuffle_seed)\n",
    "\n",
    "    ## If not square image, pad the smaller dim:\n",
    "    if input_shape[0]>input_shape[1]:\n",
    "        pad_size = input_shape[0] - input_shape[1]\n",
    "        x = np.pad(x, ((0,0),(0,0),(pad_size,0),(0,0)), 'maximum')\n",
    "        y = np.pad(y, ((0,0),(0,0),(pad_size,0),(0,0)), 'maximum')\n",
    "\n",
    "    if input_shape[0]<input_shape[1]:\n",
    "        pad_size = input_shape[1] - input_shape[0]\n",
    "        x = np.pad(x, ((0,0),(pad_size,0),(0,0),(0,0)), 'maximum')\n",
    "        y = np.pad(y, ((0,0),(pad_size,0),(0,0),(0,0)), 'maximum')\n",
    "    \n",
    "    # Split data to train and test:\n",
    "    (x_train, y_train) = x[:n_train], y[:n_train]\n",
    "    (x_test, y_test) = x[n_train:], y[n_train:]\n",
    "\n",
    "    print(f'y train size: {y_train.shape}')\n",
    "    print(f'y test size: {y_test.shape}')\n",
    "    print(f'x train size: {x_train.shape}')\n",
    "    print(f'x test size: {x_test.shape}')\n",
    "\n",
    "    # Print x normalization data:\n",
    "    # Check that x is already normalized:\n",
    "    \n",
    "    print(f'mean x_train: {np.mean(x_train)}')\n",
    "    print(f'std x_train: {np.std(x_train)}')\n",
    "    print(f'min x_train: {np.min(x_train)}')\n",
    "    print(f'max x_train: {np.max(x_train)}')\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show images - only for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(x, y, n_ims_show=5):\n",
    "    \n",
    "    # examples of the x images \n",
    "    plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = (15, 15)\n",
    "    plt.imshow(np.concatenate(x[:n_ims_show,:,:,0],axis=1), interpolation='none')\n",
    "    plt.axis('off');\n",
    "    # examples of the y images - first landmark\n",
    "    y_32 = y.astype(np.float32)\n",
    "    plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = (15, 15)\n",
    "    plt.imshow(np.concatenate(y_32[:n_ims_show,:,:,0],axis=1), interpolation='none')\n",
    "    plt.axis('off');\n",
    "    \n",
    "    # example of overlay all y on x:\n",
    "    y_overlay = np.max(y_32, axis=3)\n",
    "    xy_overlay = x[:,:,:,0] + y_overlay\n",
    "    plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = (15, 15)\n",
    "    plt.imshow(np.concatenate(xy_overlay[:n_ims_show,:,:],axis=1), interpolation='none')\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def call_model(x_train,x_test,y_train,y_test):\n",
    "def call_model(train_generator,x_test,y_test):   \n",
    "    \n",
    "    input_shape = x_test.shape[1:]\n",
    "    \n",
    "    model = cnn(input_shape, y_test.shape[-1])\n",
    "    model.summary()\n",
    "    \n",
    "    # Define folder to save results:\n",
    "    models_dir = os.path.join(input_dir, 'models')\n",
    " \n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Early stopping, saving checkpoints (only weights), reduce alpha on plateau..\n",
    "    callbacks = [\n",
    "    #        EarlyStopping(monitor='val_loss',\n",
    "    #                               patience=8,\n",
    "    #                               verbose=1,\n",
    "    #                               min_delta=1e-4),\n",
    "    #             ReduceLROnPlateau(monitor='val_loss',\n",
    "    #                               factor=0.1,\n",
    "    #                               patience=4,\n",
    "    #                               verbose=1,\n",
    "    #                               epsilon=1e-4),\n",
    "                 ModelCheckpoint(monitor='val_loss',\n",
    "                                 filepath= os.path.join(models_dir,'{epoch:04d}_{val_loss:.4f}.hdf5'),\n",
    "                                 period=20,\n",
    "                                 save_weights_only=True)]\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              verbose=2)\n",
    "    \n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the training process\n",
    "def plot_history(history):\n",
    "    print(\"Available data:\", history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    \n",
    "    #plt.figure - needed to draw more than one image in cell\n",
    "    plt.figure\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(history):\n",
    "    #pd.DataFrame(history.history).plot()\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(os.path.join(models_dir,'history.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(x, y, is_show_images=False):\n",
    "    \n",
    "    x_train,x_test,y_train,y_test = load_input_set_shuffle_and_split(x, y, is_show_images)\n",
    "\n",
    "    if is_show_images:\n",
    "        show_images(x_train, y_train)\n",
    "        \n",
    "    history = call_model(x_train,x_test,y_train,y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all(x, y, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (512,512,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn(input_shape, 40)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu notebook)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
