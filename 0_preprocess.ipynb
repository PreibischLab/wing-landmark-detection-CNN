{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess images to prepare for CNN landmark detection\n",
    "\n",
    "Requires:\n",
    "* original images\n",
    "* GT landmark positions\n",
    "* mask images (optional)\n",
    "\n",
    "Does:\n",
    "* Resizes the image to the analysis size \n",
    "* Normalizes per image\n",
    "* Masks (if available/desirable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from skimage import io\n",
    "from skimage.filters import gaussian\n",
    "import math\n",
    "from skimage.transform import resize\n",
    "import tifffile as tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "plt.set_cmap('gray')\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show landmarks on image\n",
    "def show_landmarks_on_image(im, landmarks):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    implot = plt.imshow(im)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(landmarks[:,0], landmarks[:,1], c='r', s=40)\n",
    "    plt.show()\n",
    "    sns.despine(bottom=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize function with anti-alias\n",
    "\n",
    "import skimage\n",
    "skimage.__version__\n",
    "\n",
    "# If <0.15 we need to copy the skimage.transform.resize version from version >= 15.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = '/YOURPATH/'\n",
    "\n",
    "# Size to run the analysis on:\n",
    "resize_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of landmarks:\n",
    "n_lm = 40\n",
    "\n",
    "# Define gaussian blur sizes to run the detection on (to benchmark)\n",
    "# gaussian filter w sigma 3 worked best (not significant)\n",
    "#gauss_range = [x for x in range(3,4)]\n",
    "sig = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input dirs:\n",
    "imgs_dir = os.path.join(parent_dir, 'data/images/')\n",
    "ldmks_dir = os.path.join(parent_dir, 'ldmks/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(parent_dir, 'data_CNN')\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the images and the landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_files = [f for f in glob.glob(os.path.join(imgs_dir,'*'))]\n",
    "all_landmarks_files = [f for f in glob.glob(os.path.join(ldmks_dir,'*'))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_names = [os.path.basename(f)[:-4] for f in all_images_files]\n",
    "all_landmarks_names = [os.path.basename(f)[:-4] for f in all_landmarks_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all images files have landmarks file:\n",
    "print(f'number of images: {len(all_images_names)}')\n",
    "set(all_images_names).symmetric_difference(all_landmarks_names)\n",
    "\n",
    "# another way to test this:\n",
    "#set(all_images_names) ^ set(all_landmarks_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort all lists to be on the same order\n",
    "images_lists = zip(all_images_names, all_images_files)\n",
    "landmarks_lists = zip(all_landmarks_names, all_landmarks_files)\n",
    "\n",
    "images_lists_s = list(sorted(images_lists))\n",
    "landmarks_lists_s = list(sorted(landmarks_lists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete images that can't be analysed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete if not all landmarks are found for image:\n",
    "\n",
    "# Find number of landmarks for each image:\n",
    "num_lm_list = [sum(1 for line in open(l[1])) for l in landmarks_lists_s]\n",
    "# bool that - false if number of landmarks is not correct:\n",
    "bool_lm = [1 if n==n_lm else 0 for n in num_lm_list]\n",
    "\n",
    "# Delete faulty images and landmarks list:\n",
    "images_lists_s = [im for i,im in enumerate(images_lists_s) if bool_lm[i]]\n",
    "landmarks_lists_s = [lms for i,lms in enumerate(landmarks_lists_s) if bool_lm[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All images landmarks read - to list of np arrays:\n",
    "ims_landmarks = []\n",
    "for l in landmarks_lists_s:\n",
    "    lines = [line.rstrip('\\n') for line in open(l[1])]\n",
    "    ims_landmarks.append(np.asarray([f.split(\" \") for f in lines]).astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the axes in the landmarks:\n",
    "max_x = 0\n",
    "max_y = 0\n",
    "for l in ims_landmarks:\n",
    "    mx = np.max(l[:,0])\n",
    "    my = np.max(l[:,1])\n",
    "    if mx>max_x:\n",
    "        max_x=mx\n",
    "    if my>max_y:\n",
    "        max_y=my\n",
    "        \n",
    "print(f'max landmarks: {max_x} {max_y}')\n",
    "\n",
    "# Check the axes in the images:\n",
    "from PIL import Image\n",
    "from PIL.TiffTags import TAGS\n",
    "\n",
    "with Image.open(images_lists_s[0][1]) as img:\n",
    "    im_length = [img.tag[key][0] for key in img.tag if TAGS[key]=='ImageLength'][0]\n",
    "    im_width = [img.tag[key][0] for key in img.tag if TAGS[key]=='ImageWidth'][0]\n",
    "    #meta_dict = {TAGS[key] : img.tag[key] for key in img.tag}\n",
    "    \n",
    "print(f'image dims: {im_width} {im_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete if not all landmarks are inside the image \n",
    "# (sometimes a wing is a bit outside of image, and the landmark will be outside):\n",
    "idx2rm = []\n",
    "for i,lms in enumerate(ims_landmarks):\n",
    "    #if (np.max(lms[:,0])>=(im_width-im_width/resize_size)) or (np.max(lms[:,1])>=(im_length-im_length/resize_size)):\n",
    "    if (np.max(lms[:,0])>=im_width) or (np.max(lms[:,1])>=im_length):\n",
    "        idx2rm.append(i)\n",
    "\n",
    "images_lists_s = [im for i,im in enumerate(images_lists_s) if i not in idx2rm]\n",
    "ims_landmarks = [lms for i,lms in enumerate(ims_landmarks) if i not in idx2rm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write imgs names to file \n",
    "(ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ims = len(images_lists_s)\n",
    "n_ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(map(list, zip(*images_lists_s)))[0]\n",
    "original_names_outfile = os.path.join(output_dir, 'original_files_list.txt')\n",
    "\n",
    "with open(original_names_outfile, mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Landmarks Info\n",
    "e.g distance between pairs of landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Analyse spot relationships - \n",
    "### whats the distance between the spots:\n",
    "    \n",
    "max_dis_matrix = np.zeros((40,40))\n",
    "min_dis_matrix = np.full((40,40), 4000)\n",
    "\n",
    "for wing in ims_landmarks:\n",
    "    for i in range(40):\n",
    "        for j in range(40):\n",
    "            \n",
    "            ii_dist = wing[i][0]-wing[j][0]\n",
    "            jj_dist = wing[i][1]-wing[j][1]\n",
    "        \n",
    "            dist = (math.sqrt(ii_dist**2 + jj_dist**2))\n",
    "            \n",
    "            max_dis_matrix[i,j] = dist if dist > max_dis_matrix[i,j] else max_dis_matrix[i,j]\n",
    "            min_dis_matrix[i,j] = dist if dist < min_dis_matrix[i,j] else min_dis_matrix[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dis_matrix_int = max_dis_matrix.astype(int)\n",
    "min_dis_matrix_int = min_dis_matrix.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(output_dir,'max_ldmk_dis_matrix_int'),max_dis_matrix_int)\n",
    "np.save(os.path.join(output_dir,'min_ldmk_dis_matrix_int'),min_dis_matrix_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(min_dis_matrix_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(40,80))\n",
    "\n",
    "min_val, max_val = 0, 2143\n",
    "\n",
    "a = ax[0].matshow(min_dis_matrix_int, cmap=plt.cm.Blues)\n",
    "fig.colorbar(a)\n",
    "b = ax[1].matshow(max_dis_matrix_int, cmap=plt.cm.Blues)\n",
    "fig.colorbar(b)\n",
    "\n",
    "for i in range(40):\n",
    "    for j in range(40):\n",
    "        c = min_dis_matrix_int[j,i]\n",
    "        ax[0].text(i, j, str(c), va='center', ha='center', fontsize=15)\n",
    "        \n",
    "for i in range(40):\n",
    "    for j in range(40):\n",
    "        c = max_dis_matrix_int[j,i]\n",
    "        ax[1].text(i, j, str(c), va='center', ha='center', fontsize=15)\n",
    "        \n",
    "        \n",
    "fig = plt.gcf()\n",
    "fig.savefig(f'mat_max_min.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all images and landmarks:\n",
    "Landmarks for now just as a dot image per landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save x and y as numpy arrays:\n",
    "\n",
    "## I'm only taking a 100 images as yann wants training on a small dataset \n",
    "# to see if the method can be applied on such.\n",
    "n=100\n",
    "\n",
    "X = np.zeros((n, im_length, im_width), dtype=np.float16)\n",
    "Y = np.zeros((n, im_length, im_width, n_lm), dtype='uint8')\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    # Load image:\n",
    "    im = skimage.img_as_float(io.imread(images_lists_s[i][1])).astype(np.float16)    \n",
    "    X[i,:,:] = im\n",
    "    \n",
    "    # Load Landmarks:\n",
    "    landmarks = ims_landmarks[i].copy().astype(int)\n",
    "    \n",
    "    for j,lm in enumerate(landmarks):\n",
    "        landmark_im = np.zeros(im.shape, dtype='int8')\n",
    "        landmark_im[lm[1],lm[0]] = 1\n",
    "        #landmark_im = gaussian(landmark_im,gaussian_size)\n",
    "\n",
    "        Y[i,:,:,j] = landmark_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(output_dir,f'X.npy'), X)  \n",
    "np.save(os.path.join(output_dir,f'Y.npy'), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### show landmarks on example images:\n",
    "\n",
    "nn=2\n",
    "\n",
    "for i in range(nn):\n",
    "\n",
    "    # plt doesnt support float16\n",
    "    im = X[i].astype(np.float32)\n",
    "    \n",
    "    # Load Landmarks:\n",
    "    landmarks = ims_landmarks[i].copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.imshow(im)\n",
    "    for j,lm in enumerate(landmarks):\n",
    "        \n",
    "        ax.text(lm[0],lm[1],f'{j}',color='r',fontsize=15, fontweight=\"bold\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(f'im{i}_wing_w_numbers_ldmks.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize X\n",
    "zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X)\n",
    "X_std = np.std(X, dtype=np.float64)\n",
    "print(\"X mean value is\", X_mean)\n",
    "print(\"X std value is\", X_std)\n",
    "np.max(X), np.min(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= X_mean\n",
    "X /= X_std\n",
    "# check again to double check\n",
    "print(\"After normalization the data has mean value\", np.mean(X))\n",
    "print(\"After normalization the data has standard deviation\", np.std(X, dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(output_dir,'X_normed.npy'), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(os.path.join(output_dir,'X_normed.npy'))\n",
    "Y = np.load(os.path.join(output_dir,'Y.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = X[0]\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.sum(Y[2], axis=2)\n",
    "np.where(y==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Y images for CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed load Y:\n",
    "n_ims = Y.shape[0]\n",
    "n_ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Y[2], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all Gaussians:\n",
    "(normalized between 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sig in gauss_range:\n",
    "\n",
    "Y_sig = np.zeros((n_ims, im_length, im_width, n_lm), dtype='float16')\n",
    "for i in range(n_ims):\n",
    "    for j in range(n_lm):\n",
    "        im = gaussian(Y[i,:,:,j], sig)\n",
    "        # Normalize and assign:\n",
    "        Y_sig[i,:,:,j] = im/np.max(im)\n",
    "\n",
    "np.save(os.path.join(output_dir,f'Y_sig{sig}.npy'), Y_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test it:\n",
    "\n",
    "i = 2\n",
    "\n",
    "#Y_sig = np.load(os.path.join(output_path,'Y_sig4.npy'))\n",
    "\n",
    "y = np.sum(Y_sig[i], axis=2)\n",
    "\n",
    "im = y#X[i,:,:] + y\n",
    "#im = y[200:500,2000:2500]\n",
    "plt.figure(figsize=(10,10))\n",
    "implot = plt.imshow(im.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Euclidean Distance (Vector) Image:\n",
    "that will be 2 images per landmark - x and y distances:  \n",
    "didnt work as well as gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_IJdist = np.zeros((n_ims, resize_size, resize_size, n_lm, 2), dtype=np.float32)\n",
    "# for i in range(n_ims):\n",
    "#     landmarks = ims_landmarks[i].copy()\n",
    "    \n",
    "#     landmarks[:,1] += 606\n",
    "#     landmarks = landmarks/20\n",
    "    \n",
    "#     for j,lm in enumerate(landmarks):\n",
    "        \n",
    "#         Y_IJdist[i,:,:,j,0] = np.fromfunction(lambda ii,jj: lm[1]-ii, (resize_size, resize_size), dtype=np.float32)\n",
    "#         Y_IJdist[i,:,:,j,1] = np.fromfunction(lambda ii,jj: lm[0]-jj, (resize_size, resize_size), dtype=np.float32)\n",
    "        \n",
    "# np.save(os.path.join(output_dir,f'Y_IJdist.npy'), Y_IJdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test it:\n",
    "\n",
    "# i = 2\n",
    "\n",
    "# #Y_sig = np.load(os.path.join(output_dir,'Y_sig4.npy'))\n",
    "\n",
    "# y = np.zeros((resize_size,resize_size))\n",
    "# for j in range(1):\n",
    "#     y[:,:] = y + Y_IJdist[i,:,:,j,0] #+ Y_IJdist[i,:,:,j,1]\n",
    "\n",
    "# im = X[i,:,:] + y\n",
    "# plt.figure(figsize=(10,10))\n",
    "# implot = plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make between -1 - 1\n",
    "# Y_IJdist_normed = np.zeros((n_ims, resize_size, resize_size, n_lm, 2), dtype=np.float16)\n",
    "\n",
    "# for i in range(n_ims):\n",
    "#     for j,lm in enumerate(landmarks):\n",
    "#         im_ii = Y_IJdist[i,:,:,j,0]\n",
    "#         Y_IJdist_normed[i,:,:,j,0] = im_ii/np.max(np.absolute(im_ii))\n",
    "#         im_jj = Y_IJdist[i,:,:,j,1]\n",
    "#         Y_IJdist_normed[i,:,:,j,1] = im_jj/np.max(np.absolute(im_jj))\n",
    "        \n",
    "# np.save(os.path.join(output_dir,f'Y_IJdist_normed.npy'), Y_IJdist_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge Dims:\n",
    "# Y_IJdist_normed = np.load(os.path.join(output_dir,'Y_IJdist_normed.npy'))\n",
    "# Y_IJdist_normed_80 = np.zeros((n_ims, resize_size, resize_size, n_lm*2), dtype=np.float32)\n",
    "\n",
    "# for i in range(n_ims):\n",
    "#     for j in range(n_lm):\n",
    "#         Y_IJdist_normed_80[i,:,:,j*2] = Y_IJdist_normed[i,:,:,j,0]\n",
    "#         Y_IJdist_normed_80[i,:,:,j*2+1] = Y_IJdist_normed[i,:,:,j,1]\n",
    "        \n",
    "# np.save(os.path.join(output_dir,f'Y_IJdist_normed_80.npy'), Y_IJdist_normed_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ~zero to 1 or -1, 1 and -1 to zero\n",
    "# Y_IJdist_normed_flipped = np.where(Y_IJdist_normed<=0, Y_IJdist_normed+1, 10)\n",
    "# Y_IJdist_normed_flipped = np.where(Y_IJdist_normed>0, Y_IJdist_normed-1, Y_IJdist_normed_flipped)\n",
    "\n",
    "# np.save(os.path.join(output_dir,f'Y_IJdist_normed_flipped.npy'), Y_IJdist_normed_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge Dims:\n",
    "# Y_IJdist_normed_flipped = np.load(os.path.join(output_dir,'Y_IJdist_normed_flipped.npy'))\n",
    "# Y_IJdist_normed_flipped_80 = np.zeros((n_ims, resize_size, resize_size, n_lm*2), dtype=np.float32)\n",
    "\n",
    "# for i in range(n_ims):\n",
    "#     for j in range(n_lm):\n",
    "#         Y_IJdist_normed_flipped_80[i,:,:,j*2] = Y_IJdist_normed_flipped[i,:,:,j,0]\n",
    "#         Y_IJdist_normed_flipped_80[i,:,:,j*2+1] = Y_IJdist_normed_flipped[i,:,:,j,1]\n",
    "        \n",
    "# np.save(os.path.join(output_dir,f'Y_IJdist_normed_flipped_80.npy'), Y_IJdist_normed_flipped_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ABS\n",
    "# Y_IJdist_normed_flipped_abs = np.absolute(Y_IJdist_normed_flipped)\n",
    "# np.save(os.path.join(output_dir,f'Y_IJdist_normed_flipped_abs.npy'), Y_IJdist_normed_flipped_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ABS merge dims:\n",
    "\n",
    "# Y_IJdist_normed_flipped_abs_80 = np.zeros((n_ims, resize_size, resize_size, n_lm*2), dtype=np.float32)\n",
    "\n",
    "# for i in range(n_ims):\n",
    "#     for j,lm in enumerate(landmarks):\n",
    "#         Y_IJdist_normed_flipped_abs_80[i,:,:,j*2] = Y_IJdist_normed_flipped_abs[i,:,:,j,0]\n",
    "#         Y_IJdist_normed_flipped_abs_80[i,:,:,j*2+1] = Y_IJdist_normed_flipped_abs[i,:,:,j,1]\n",
    "        \n",
    "        \n",
    "# np.save(os.path.join(output_dir,f'Y_IJdist_normed_flipped_abs_80.npy'), Y_IJdist_normed_flipped_abs_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make euclidean distance images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_IJdist = np.load(os.path.join(output_dir,'Y_IJdist.npy'))\n",
    "# Y_eucli = np.zeros((n_ims, resize_size, resize_size, n_lm), dtype=np.float32)\n",
    "\n",
    "# for i in range(n_ims):\n",
    "#     for j in range(n_lm):\n",
    "#         Y_eucli[i,:,:,j] = np.sqrt(np.square(Y_IJdist[i,:,:,j,0]) + np.square(Y_IJdist[i,:,:,j,1]))\n",
    "\n",
    "# Y_eucli_normed = np.zeros((n_ims, resize_size, resize_size, n_lm), dtype=np.float32)\n",
    "# for i in range(n_ims):\n",
    "#     for j in range(n_lm):\n",
    "#         im = Y_eucli[i,:,:,j]\n",
    "#         Y_eucli_normed[i,:,:,j] = im/np.max(im)\n",
    "        \n",
    "# np.save(os.path.join(output_dir,f'Y_eucli_normed.npy'), Y_eucli_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ~zero to 1, 1 to zero\n",
    "# Y_eucli_normed_flipped = np.absolute(Y_eucli_normed-1)\n",
    "\n",
    "# np.save(os.path.join(output_dir,f'Y_eucli_normed_flipped.npy'), Y_eucli_normed_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Y_eucli_normed_flipped[3,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only neighborhood (biggest values):\n",
    "# Y_eucli_normed_flipped_hood = np.copy(Y_eucli_normed_flipped)\n",
    "# Y_eucli_normed_flipped_hood[Y_eucli_normed_flipped_hood<0.9] = 0\n",
    "\n",
    "# np.save(os.path.join(output_dir,f'Y_eucli_normed_flipped_hood.npy'), Y_eucli_normed_flipped_hood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make one euclidean distance image - the nearest point:\n",
    "# Y_eucli_normed_flipped_closest_1 = np.zeros((n_ims, resize_size, resize_size), dtype=np.float32)\n",
    "\n",
    "# for i in range(n_ims):\n",
    "#     Y_eucli_normed_flipped_closest_1[i,:,:] = np.max(Y_eucli_normed_flipped[i,:,:,:], axis=2)\n",
    "    \n",
    "# np.save(os.path.join(output_dir,f'Y_eucli_normed_flipped_closest_1.npy'), Y_eucli_normed_flipped_closest_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Y_eucli_normed_flipped_closest_1[3,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take only the neighborhood (biggest values):\n",
    "# Y_eucli_normed_flipped_closest_hood_1 = np.copy(Y_eucli_normed_flipped_closest_1)\n",
    "# Y_eucli_normed_flipped_closest_hood_1[Y_eucli_normed_flipped_closest_hood_1<0.9] = 0\n",
    "\n",
    "# np.save(os.path.join(output_dir,f'Y_eucli_normed_flipped_closest_hood_1.npy'), Y_eucli_normed_flipped_closest_hood_1)\n",
    "# plt.imshow(Y_eucli_normed_flipped_closest_hood_1[3,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create masked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(os.path.join(output_dir, 'X_normed.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = x.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(original_names_outfile,'r') as f:\n",
    "    filenames = (f.read()).split(\"\\n\")\n",
    "    \n",
    "filenames = filenames[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_path = os.path.join(output_dir,'..','data','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = np.asarray([io.imread(os.path.join(masks_path, f'{f}.tif')) for f in filenames], dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[masks] = np.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[11].astype(np.float32))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(output_dir,f'X_masked.npy'), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_name = 'X_masked.npy'\n",
    "Y_name = 'Y_sig3.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(os.path.join(input_dir, X_name))\n",
    "y = np.load(os.path.join(input_dir, Y_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "x = x[:n]\n",
    "y = y[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siz = (1724,2048)\n",
    "x_resized = np.zeros((n,siz[0],siz[1]), dtype=np.float16)\n",
    "for i,im in enumerate(x):\n",
    "    x_resized[i] = resize(im.astype(np.float32), siz, anti_aliasing=True, preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siz = (1724,2048)\n",
    "y_resized = np.zeros((n,siz[0],siz[1],y.shape[-1]), dtype=np.float16)\n",
    "\n",
    "for i in range(y.shape[0]):\n",
    "    for j in range(y.shape[-1]):\n",
    "        y_resized[i,:,:,j] = resize(y[i,:,:,j].astype(np.float32), siz, anti_aliasing=True, preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(input_dir, f'X_masked_resized_{siz[0]}_{siz[1]}'), x_resized)\n",
    "np.save(os.path.join(input_dir, f'Y_sig3_resized_{siz[1]}_{siz[1]}'), y_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
